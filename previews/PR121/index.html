<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · AugmentedGPLikelihoods.jl</title><script data-outdated-warner src="assets/warner.js"></script><link rel="canonical" href="https://JuliaGaussianProcesses.github.io/AugmentedGPLikelihoods.jl/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>AugmentedGPLikelihoods.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#The-augmentation"><span>The augmentation</span></a></li><li><a class="tocitem" href="#Bayesian-Inference"><span>Bayesian Inference</span></a></li><li><a class="tocitem" href="#What-this-package-does"><span>What this package does</span></a></li><li><a class="tocitem" href="#gibbs-sampling-index"><span>Gibbs Sampling</span></a></li><li><a class="tocitem" href="#Coordinate-Ascent-Variational-Inference"><span>Coordinate Ascent Variational Inference</span></a></li><li><a class="tocitem" href="#Likelihood-and-ELBO-computations"><span>Likelihood and ELBO computations</span></a></li></ul></li><li><span class="tocitem">Likelihoods</span><ul><li><a class="tocitem" href="likelihoods/bernoulli/">Bernoulli Likelihood (Logistic Link)</a></li><li><a class="tocitem" href="likelihoods/categorical/">Categorical Likelihood (LogisticSoftmax Link)</a></li><li><a class="tocitem" href="likelihoods/laplace/">Laplace Likelihood</a></li><li><a class="tocitem" href="likelihoods/negativebinomial/">Negative Binomial Likelihood (Logistic Link)</a></li><li><a class="tocitem" href="likelihoods/poisson/">Poisson Likelihood (Scaled Logistic Link)</a></li><li><a class="tocitem" href="likelihoods/studentt/">Student-T Likelihood</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="examples/bernoulli/">Binary Classification</a></li><li><a class="tocitem" href="examples/categorical/">Multi-Class Classification</a></li><li><a class="tocitem" href="examples/laplace/">Regression with Laplace noise</a></li><li><a class="tocitem" href="examples/negativebinomial/">Event counting with Negative Binomial</a></li><li><a class="tocitem" href="examples/poisson/">Poisson regression</a></li><li><a class="tocitem" href="examples/studentt/">Regression with Student-t noise</a></li></ul></li><li><a class="tocitem" href="misc/">Misc</a></li><li><a class="tocitem" href="references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/main/docs/src/index.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="AugmentedGPLikelihoods"><a class="docs-heading-anchor" href="#AugmentedGPLikelihoods">AugmentedGPLikelihoods</a><a id="AugmentedGPLikelihoods-1"></a><a class="docs-heading-anchor-permalink" href="#AugmentedGPLikelihoods" title="Permalink"></a></h1><p>Documentation for <a href="https://github.com/theogf/AugmentedGPLikelihoods.jl">AugmentedGPLikelihoods</a>.</p><p>Gaussian Processes (GPs) are great tools for working with function approximations including uncertainty. On top of Gaussian regression, GPs can be used as latent functions for a lot of different tasks such as non-Gaussian regression, classification, multi-class classification or event counting. However, these tasks involve non-conjugate likelihoods and the GP posterior  is then intractable. The typical solution is to approximate the posterior by either sampling from it or approximating it with another distribution. However, both these methods are computationally involved, require gradient and are not always guaranteed to converge.</p><h2 id="The-augmentation"><a class="docs-heading-anchor" href="#The-augmentation">The augmentation</a><a id="The-augmentation-1"></a><a class="docs-heading-anchor-permalink" href="#The-augmentation" title="Permalink"></a></h2><p>An alternative proposed in <a href="references/#galy20">[1]</a> is to represent these non-conjugate likelihoods as scale-mixtures (sometimes requiring multiple steps) to obtain a <strong>conditionally conjugate likelihood</strong>. More concretely, some likelihoods can be written as:</p><p class="math-container">\[    p(x) = \int q(x|\omega)d\pi(\omega).\]</p><p>One can then get rid of the integral by <strong>augmenting</strong> the model with the auxiliary variable <span>$\omega$</span>. With the right choice of mixture, one can obtain a likelihood conjugate with a GP <span>$f$</span> but only when conditioned on <span>$\omega$</span>. This means we go from</p><p class="math-container">\[    p(f|y) = \frac{p(y|f)p(f)}{p(y)},\]</p><p>which is intractable, to</p><p class="math-container">\[    p(f,\omega|y) = \frac{p(y,\omega|f)p(f)}{p(y)},\]</p><p>where the conditionals <span>$p(f|\omega,y)$</span> and <span>$p(\omega|f,y)$</span> are known in closed-form.</p><h2 id="Bayesian-Inference"><a class="docs-heading-anchor" href="#Bayesian-Inference">Bayesian Inference</a><a id="Bayesian-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Inference" title="Permalink"></a></h2><p>This new formulation leads to easier, more stable and faster inference. A natural algorithm following from this formulation is the <a href="https://en.wikipedia.org/wiki/Gibbs_sampling#Blocked_Gibbs_sampler"><strong>Blocked Gibbs Sampling</strong></a> but also the Coordinate Ascent VI (CAVI) algorithm where the conditionals are used to compute the optimal variational distribution.</p><h2 id="What-this-package-does"><a class="docs-heading-anchor" href="#What-this-package-does">What this package does</a><a id="What-this-package-does-1"></a><a class="docs-heading-anchor-permalink" href="#What-this-package-does" title="Permalink"></a></h2><p>Although an automatic way is proposed in <a href="references/#galy20">[1]</a>, most of the augmentations require some hand derivations. This package provides the results of these derivations (as well as the derivations) and propose a unified framework to obtain the distributions of interest. Generally values (either samples or distributions) of the auxiliary variables need to be carried around. Since, each likelihood can have a different structure of auxiliary variables, they are uniformly moved as <span>$\Omega$</span>, which is a <code>NamedTuple</code> containing <code>Vector</code>s of either samples or distributions.</p><h2 id="gibbs-sampling-index"><a class="docs-heading-anchor" href="#gibbs-sampling-index">Gibbs Sampling</a><a id="gibbs-sampling-index-1"></a><a class="docs-heading-anchor-permalink" href="#gibbs-sampling-index" title="Permalink"></a></h2><p>We give an example in the Gibbs Sampling tutorial using <code>AbstractMCMC</code>. But the API can be reduced to 5 main functions:</p><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.init_aux_variables" href="#AugmentedGPLikelihoods.init_aux_variables"><code>AugmentedGPLikelihoods.init_aux_variables</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">init_aux_variables([rng::AbstractRNG], ::Likelihood, n::Int) -&gt; TupleVector</code></pre><p>Initialize collections of <code>n</code> auxiliary variables in  a <code>TupleVector</code> to be used in the context of sampling. <code>n</code> should be the number of data inputs.</p><p>See also <a href="#AugmentedGPLikelihoods.init_aux_posterior"><code>init_aux_posterior</code></a> for variational inference.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.aux_sample!" href="#AugmentedGPLikelihoods.aux_sample!"><code>AugmentedGPLikelihoods.aux_sample!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">aux_sample!([rng::AbstractRNG], Ω, lik::Likelihood, y, f) -&gt; TupleVector</code></pre><p>Sample the auxiliary variables <code>Ω</code> in-place based on the full-conditional <a href="#AugmentedGPLikelihoods.aux_full_conditional"><code>aux_full_conditional</code></a> associated with the augmented likelihood:</p><p class="math-container">\[    p(\Omega|y,f) \propto p(\Omega,y|f).\]</p><p>See also <a href="#AugmentedGPLikelihoods.aux_sample"><code>aux_sample</code></a> for an allocating version and <a href="#AugmentedGPLikelihoods.aux_posterior!"><code>aux_posterior!</code></a> for variational inference.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L31-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.aux_sample" href="#AugmentedGPLikelihoods.aux_sample"><code>AugmentedGPLikelihoods.aux_sample</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">aux_sample([rng::AbstractRNG], lik::Likelihood, y, f) -&gt; TupleVector</code></pre><p>Sample and allocate the auxiliary variables <code>Ω</code> in a <code>TupleVector</code> based on the full-conditional associated with the likelihood.</p><p>See <a href="#AugmentedGPLikelihoods.aux_sample!"><code>aux_sample!</code></a> for an in-place version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L45-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.aux_full_conditional" href="#AugmentedGPLikelihoods.aux_full_conditional"><code>AugmentedGPLikelihoods.aux_full_conditional</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">aux_full_conditional(lik::AbstractLikelihood, yᵢ, fᵢ) -&gt; AbstractNTDist</code></pre><p>Return the full conditional distribution of <code>Ωᵢ</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L24-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.auglik_potential" href="#AugmentedGPLikelihoods.auglik_potential"><code>AugmentedGPLikelihoods.auglik_potential</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">auglik_potential(lik::Likelihood, Ω, y) -&gt; Tuple</code></pre><p>Given the augmented likelihood <span>$l(\Omega,y,f) \propto \exp(\beta(\Omega,y) f + \gamma(\Omega,y)f^2)$</span>, return the potential, <span>$\beta(\Omega,y)$</span>. Note that this equivalent to the shift of the first natural parameter <span>$\eta_1 = \Sigma^{-1}\mu$</span>. The <code>Tuple</code> contains a <code>Vector</code> for each latent.</p><p>See also <a href="#AugmentedGPLikelihoods.expected_auglik_potential"><code>expected_auglik_potential</code></a> for variational inference.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L86-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.auglik_precision" href="#AugmentedGPLikelihoods.auglik_precision"><code>AugmentedGPLikelihoods.auglik_precision</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">auglik_precision(lik::Likelihood, Ω, y) -&gt; Tuple</code></pre><p>Given the augmented likelihood <span>$l(\Omega,y,f) \propto \exp(\beta(\Omega,y) f + \frac{\gamma(\Omega,y)}{2}f^2)$</span>, return the precision, <span>$\gamma(\Omega,y)$</span>, note that this equivalent to the shift of the precision <span>$\Lambda = \Sigma^{-1}$</span>. The <code>Tuple</code> contains a <code>Vector</code> for each latent.</p><p>See also <a href="#AugmentedGPLikelihoods.expected_auglik_precision"><code>expected_auglik_precision</code></a> for variational inference.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L98-L107">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.auglik_potential_and_precision" href="#AugmentedGPLikelihoods.auglik_potential_and_precision"><code>AugmentedGPLikelihoods.auglik_potential_and_precision</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">auglik_potential_and_precision(lik::Likelihood, Ω, y) -&gt; Tuple{Tuple, Tuple}</code></pre><p>Returns both <a href="#AugmentedGPLikelihoods.auglik_potential"><code>auglik_potential</code></a> and <a href="#AugmentedGPLikelihoods.auglik_precision"><code>auglik_precision</code></a> when some  computation can be saved by doing both at the same time.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L110-L115">source</a></section></article><p>First <a href="#AugmentedGPLikelihoods.init_aux_variables"><code>init_aux_variables</code></a> initializes a <code>NamedTuple</code> of <code>Vector</code>(s) of auxiliary variables to be modified in-place during sampling. <a href="#AugmentedGPLikelihoods.aux_sample!"><code>aux_sample!</code></a> will sample the auxiliary variables from the full conditional <span>$p(\Omega|y,f)$</span>, given by <a href="#AugmentedGPLikelihoods.aux_full_conditional"><code>aux_full_conditional</code></a>, and return the modified <code>NamedTuple</code>. For generating a new <code>NamedTuple</code> every time, see <a href="#AugmentedGPLikelihoods.aux_sample"><code>aux_sample</code></a>. The full-conditional from <span>$f$</span> are given by</p><p class="math-container">\[\begin{align*}
    p(f|y,\Omega) =&amp; \mathcal{N}(f|m,S)\\
    S =&amp; \left(K^{-1} + \operatorname{Diagonal}(\lambda)\right)^{-1}\\
    m =&amp; S \left(h + K^{-1}\mu_0\right),
\end{align*}\]</p><p>where <span>$\lambda$</span> is obtained via <a href="#AugmentedGPLikelihoods.auglik_precision"><code>auglik_precision</code></a> and <span>$h$</span> is obtained via <a href="#AugmentedGPLikelihoods.auglik_potential"><code>auglik_potential</code></a>. For likelihoods requiring multiple latent GP (e.g. multi-class classification or heteroscedastic likelihoods), <a href="#AugmentedGPLikelihoods.auglik_potential"><code>auglik_potential</code></a> and <a href="#AugmentedGPLikelihoods.auglik_precision"><code>auglik_precision</code></a> return a <code>Tuple</code> with the respective <code>Vector</code>s <span>$\lambda$</span> and <span>$h$</span>.</p><p>As a general rule, the augmented likelihood will have the form</p><p class="math-container">\[    p(y|f,\Omega) \propto \exp\left(h(\Omega,y)f + \frac{\lambda(\Omega,y)}{2}f^2\right),\]</p><p>with <a href="#AugmentedGPLikelihoods.auglik_potential"><code>auglik_potential</code></a> <span>$\equiv h(\Omega,y)$</span> while <a href="#AugmentedGPLikelihoods.auglik_precision"><code>auglik_precision</code></a> <span>$\equiv \lambda(\Omega,y)$</span>.</p><h2 id="Coordinate-Ascent-Variational-Inference"><a class="docs-heading-anchor" href="#Coordinate-Ascent-Variational-Inference">Coordinate Ascent Variational Inference</a><a id="Coordinate-Ascent-Variational-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Coordinate-Ascent-Variational-Inference" title="Permalink"></a></h2><p><a href="https://en.wikipedia.org/wiki/Coordinate_descent">CAVI</a> updates work exactly the same way as the Gibbs Sampling, except we are now working with posterior distributions instead of samples. We work with the variational family <span>$q(f)\prod_{i=1}^N q(\Omega_i)$</span>, i.e. we make the <a href="https://en.wikipedia.org/wiki/Mean-field_theory"><strong>mean-field</strong></a> assumption that our new auxiliary variables are independent of each other and independent of <span>$f$</span>.</p><p>Like for <a href="#gibbs-sampling-index">Gibbs Sampling</a>, there are also 5 main functions</p><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.init_aux_posterior" href="#AugmentedGPLikelihoods.init_aux_posterior"><code>AugmentedGPLikelihoods.init_aux_posterior</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">init_aux_posterior([T::DataType], lik::Likelihood, n::Int) -&gt; AbstractProductMeasure</code></pre><p>Initialize collections of <code>n</code> (independent) posteriors for the auxiliary variables in the context of variational inference. <code>n</code> should be the size of the data used every iteration. The real variational parameters will be given type <code>T</code> (<code>Float64</code> by default)</p><p>See also <a href="#AugmentedGPLikelihoods.init_aux_variables"><code>init_aux_variables</code></a> for sampling.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L12-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.aux_posterior!" href="#AugmentedGPLikelihoods.aux_posterior!"><code>AugmentedGPLikelihoods.aux_posterior!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">aux_posterior!(qΩ, lik::Likelihood, y, qf) -&gt; AbstractProductMeasure</code></pre><p>Compute the optimal posterior of the auxiliary variables <span>$q^*(\Omega)$</span> given  the marginal distributions <code>qf</code> by updating the variational parameters in-place using the formula</p><p class="math-container">\[    q^*(\Omega) \propto \exp\left(E_{q(f)}\left[p(\Omega|f,y)\right]\right)\]</p><p>See also <a href="#AugmentedGPLikelihoods.aux_posterior"><code>aux_posterior</code></a> and <a href="#AugmentedGPLikelihoods.aux_sample!"><code>aux_sample!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L62-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.aux_posterior" href="#AugmentedGPLikelihoods.aux_posterior"><code>AugmentedGPLikelihoods.aux_posterior</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">aux_posterior(lik::Likelihood, y, qf) -&gt; AbstractProductMeasure</code></pre><p>Compute the optimal posterior of the auxiliary variables in a new <code>AbstractProductMeasure</code> (<code>For</code> by default).</p><p>See <a href="#AugmentedGPLikelihoods.aux_posterior!"><code>aux_posterior!</code></a> for more details</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L76-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.expected_auglik_potential" href="#AugmentedGPLikelihoods.expected_auglik_potential"><code>AugmentedGPLikelihoods.expected_auglik_potential</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">expected_auglik_potential(lik::Likelihood, qΩ, y) -&gt; Tuple</code></pre><p>Given the augmented likelihood <span>$l(\Omega,y,f) \propto \exp(\beta(\Omega,y) f + \frac{\gamma(\Omega,y)}{2}f^2)$</span>, return the expected potential, <span>$E_{q(\Omega)}[\beta(\Omega,y)]$</span>, note that this equivalent to the shift of the first variational natural parameter <span>$\eta_1 = \Sigma^{-1}\mu$</span>. The <code>Tuple</code> contains a <code>Vector</code> for each latent.</p><p>See also <a href="#AugmentedGPLikelihoods.auglik_potential"><code>auglik_potential</code></a> for sampling.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L118-L127">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.expected_auglik_precision" href="#AugmentedGPLikelihoods.expected_auglik_precision"><code>AugmentedGPLikelihoods.expected_auglik_precision</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">expected_auglik_precision(lik::Likelihood, qΩ, y) -&gt; Tuple</code></pre><p>Given the augmented likelihood <span>$l(\Omega,y,f) \propto \exp(\beta(\Omega,y) f + \frac{\gamma(\Omega,y)}{2}f^2)$</span>, return the expected precision, <span>$E_{q(\Omega)}[\gamma(\Omega,y)]$</span>, note that this equivalent to the shift of the variational precision <span>$\Lambda = \Sigma^{-1}$</span>. The <code>Tuple</code> contains a <code>Vector</code> for each latent.</p><p>See also <a href="#AugmentedGPLikelihoods.auglik_precision"><code>auglik_precision</code></a> for sampling.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L130-L139">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.expected_auglik_potential_and_precision" href="#AugmentedGPLikelihoods.expected_auglik_potential_and_precision"><code>AugmentedGPLikelihoods.expected_auglik_potential_and_precision</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">expected_auglik_potential_and_precision(lik::Likelihood, Ω, y) -&gt; Tuple{Tuple, Tuple}</code></pre><p>Returns both <a href="#AugmentedGPLikelihoods.expected_auglik_potential"><code>expected_auglik_potential</code></a> and <a href="#AugmentedGPLikelihoods.expected_auglik_precision"><code>expected_auglik_precision</code></a> when some  computation can be saved by doing both at the same time.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L142-L147">source</a></section></article><p><a href="#AugmentedGPLikelihoods.init_aux_posterior"><code>init_aux_posterior</code></a> initializes the posterior <span>$\prod_{i=1}^Nq(\Omega_i)$</span> as a <code>NamedTuple</code>. <a href="#AugmentedGPLikelihoods.aux_posterior!"><code>aux_posterior!</code></a> updates the variational posterior distributions in-place given the marginals <span>$q(f_i)$</span> and return the modified <code>NamedTuple</code>. To get a new <code>NamedTuple</code> every time, use <a href="#AugmentedGPLikelihoods.aux_posterior"><code>aux_posterior</code></a>. Finally, <a href="#AugmentedGPLikelihoods.expected_auglik_potential"><code>expected_auglik_potential</code></a> and <a href="#AugmentedGPLikelihoods.expected_auglik_precision"><code>expected_auglik_precision</code></a> give us the elements needed to update the variational distribution <span>$q(f)$</span>. Like for <a href="#gibbs-sampling-index">Gibbs Sampling</a>, we have the following optimal variational distributions:</p><p class="math-container">\[\begin{align*}
    q^*(f) =&amp; \mathcal{N}(f|m,S)\\
    S =&amp; \left(K^{-1} + \operatorname{Diagonal}(\lambda)\right)^{-1}\\
    m =&amp; S \left(h + K^{-1}\mu_0\right)
\end{align*}\]</p><p>where <span>$\lambda$</span> is given by <a href="#AugmentedGPLikelihoods.expected_auglik_precision"><code>expected_auglik_precision</code></a> and <span>$h$</span> is given by <a href="#AugmentedGPLikelihoods.expected_auglik_potential"><code>expected_auglik_potential</code></a>. Note that if you work with <strong>Sparse</strong> GPs, the updates should be:</p><p class="math-container">\[\begin{align*}
    S =&amp; \left(K_{Z}^{-1} + \kappa\operatorname{Diagonal}(r)\kappa^\top\right)^{-1},\\
    m =&amp; S \left(\kappa t + K_Z^{-1}\mu_0(Z)\right),
\end{align*}\]</p><p>where <span>$\kappa=K_{Z}^{-1}K_{Z,X}$</span>.</p><h2 id="Likelihood-and-ELBO-computations"><a class="docs-heading-anchor" href="#Likelihood-and-ELBO-computations">Likelihood and ELBO computations</a><a id="Likelihood-and-ELBO-computations-1"></a><a class="docs-heading-anchor-permalink" href="#Likelihood-and-ELBO-computations" title="Permalink"></a></h2><p>You might be interested in computing the <a href="https://en.wikipedia.org/wiki/Evidence_lower_bound">ELBO</a>, i.e. the lower bound on the evidence of the augmented model. The ELBO can be written as:</p><p class="math-container">\[\begin{align*}
    \mathcal{L(q(f,\Omega)} =&amp; E_{q(f)q(\Omega)}\left[\log p(y,\Omega|f)\right] - \operatorname{KL}\left(q(f)||p(f)\right)\\
    =&amp; E_{q(f)q(\Omega)}\left[\log p(y|\Omega,f)\right] - \operatorname{KL}\left(q(\Omega)||p(\Omega)\right) -\operatorname{KL}\left(q(f)||p(f)\right)
\end{align*}\]</p><p>To work with all these terms, <code>AugmentedGPLikelihoods.jl</code> provide a series of helping functions:</p><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.aug_loglik" href="#AugmentedGPLikelihoods.aug_loglik"><code>AugmentedGPLikelihoods.aug_loglik</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">aug_loglik(lik::Likelihood, Ω, y, f) -&gt; Real</code></pre><p>Return the augmented log-likelihood with the given parameters. The augmented log-likelihood is of the form </p><p class="math-container">\[    \log p(y,\Omega|f) = \log l(y,\Omega,f) + \log p(\Omega|y).\]</p><p>To only obtain the <span>$p(\Omega|y)$</span> part see <a href="#AugmentedGPLikelihoods.aux_prior"><code>aux_prior</code></a> and see <a href="#AugmentedGPLikelihoods.logtilt"><code>logtilt</code></a> for <span>$\log l(y, \Omega, f)$</span>.</p><p>A generic fallback exists based on <a href="#AugmentedGPLikelihoods.logtilt"><code>logtilt</code></a> and <a href="#AugmentedGPLikelihoods.aux_prior"><code>aux_prior</code></a> but specialized implementations are encouraged.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L150-L163">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.aux_prior" href="#AugmentedGPLikelihoods.aux_prior"><code>AugmentedGPLikelihoods.aux_prior</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">aux_prior(lik::Likelihood, y) -&gt; AbstractProductMeasure</code></pre><p>Returns a <code>NamedTuple</code> of distributions with the same structure as <a href="#AugmentedGPLikelihoods.aux_posterior"><code>aux_posterior</code></a>, <a href="#AugmentedGPLikelihoods.init_aux_posterior"><code>init_aux_posterior</code></a> and <a href="#AugmentedGPLikelihoods.init_aux_variables"><code>init_aux_variables</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L176-L181">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.aux_kldivergence" href="#AugmentedGPLikelihoods.aux_kldivergence"><code>AugmentedGPLikelihoods.aux_kldivergence</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">aux_kldivergence(lik::Likelihood, qΩ::For, pΩ::For) -&gt; Real
aux_kldivergence(lik::Likelihood, qΩ::For, y) -&gt; Real</code></pre><p>Compute the analytical KL divergence between the auxiliary variables posterior <span>$q(\Omega)$</span>, obtained with <a href="#AugmentedGPLikelihoods.aux_posterior"><code>aux_posterior</code></a> and prior <span>$p(\Omega)$</span>, obtained with <a href="#AugmentedGPLikelihoods.aux_prior"><code>aux_prior</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L166-L173">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.logtilt" href="#AugmentedGPLikelihoods.logtilt"><code>AugmentedGPLikelihoods.logtilt</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">logtilt(lik::Likelihood, Ω, y, f) -&gt; Real</code></pre><p>Compute the quadratic part on <span>$f$</span> of the augmented likelihood:</p><p class="math-container">\[    \log C(\Omega, y) + \alpha(\Omega,y) + \beta(\Omega,y)f + \frac{\gamma(\Omega,y)}{2}f^2.\]</p><p>See also <a href="#AugmentedGPLikelihoods.expected_logtilt"><code>expected_logtilt</code></a> for variational inference.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L184-L193">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGPLikelihoods.expected_logtilt" href="#AugmentedGPLikelihoods.expected_logtilt"><code>AugmentedGPLikelihoods.expected_logtilt</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">expected_logtilt(lik::Likelihood, qΩ, y, qf) -&gt; Real</code></pre><p>Compute the expectation of the quadratic part on <span>$f$</span> of the augmented likelihood.</p><p class="math-container">\[    E_{q(\Omega,f)}\left[\log C(\Omega, y) + \alpha(\Omega,y) + \beta(\Omega,y)f + \frac{\gamma(\Omega,y)}{2}f^2\right].\]</p><p>See also <a href="#AugmentedGPLikelihoods.logtilt"><code>logtilt</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/f2dc02fdb86f023c8e7f4f187ee275020d693745/src/api.jl#L196-L205">source</a></section></article><p><a href="#AugmentedGPLikelihoods.aug_loglik"><code>aug_loglik</code></a> returns the augmented log-likelihood <span>$\log p(y,\Omega|f)$</span>, but one should avoid using it as computing <span>$p(\Omega)$</span> can be expensive. <a href="#AugmentedGPLikelihoods.aux_prior"><code>aux_prior</code></a> returns the prior on the auxiliary variables, note that it can depend on the observations <span>$y$</span>. <a href="#AugmentedGPLikelihoods.logtilt"><code>logtilt</code></a> returns the log of the exponential part of the augmented likelihood, which is conjugate with <span>$f$</span>. <a href="#AugmentedGPLikelihoods.aug_loglik"><code>aug_loglik</code></a> is computed by default using <a href="#AugmentedGPLikelihoods.logtilt"><code>logtilt</code></a> + <code>logpdf(aux_prior, x)</code>. Finally, for variational inference purposes, <a href="#AugmentedGPLikelihoods.aux_kldivergence"><code>aux_kldivergence</code></a> computes the KL divergence <span>$\operatorname{KL}(q(\Omega)||p(\Omega))$</span> and <a href="#AugmentedGPLikelihoods.expected_logtilt"><code>expected_logtilt</code></a> computes the expectation of [<code>logtilt</code>] analytically.</p></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="likelihoods/bernoulli/">Bernoulli Likelihood (Logistic Link) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Sunday 24 September 2023 14:07">Sunday 24 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
