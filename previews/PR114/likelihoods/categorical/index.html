<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Categorical Likelihood (LogisticSoftmax Link) · AugmentedGPLikelihoods.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://JuliaGaussianProcesses.github.io/AugmentedGPLikelihoods.jl/likelihoods/categorical/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">AugmentedGPLikelihoods.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Likelihoods</span><ul><li><a class="tocitem" href="../bernoulli/">Bernoulli Likelihood (Logistic Link)</a></li><li class="is-active"><a class="tocitem" href>Categorical Likelihood (LogisticSoftmax Link)</a><ul class="internal"><li><a class="tocitem" href="#The-augmentation"><span>The augmentation</span></a></li><li><a class="tocitem" href="#Conditional-distributions-(Sampling)"><span>Conditional distributions (Sampling)</span></a></li><li><a class="tocitem" href="#Variational-distributions-(Variational-Inference)"><span>Variational distributions (Variational Inference)</span></a></li></ul></li><li><a class="tocitem" href="../laplace/">Laplace Likelihood</a></li><li><a class="tocitem" href="../negativebinomial/">Negative Binomial Likelihood (Logistic Link)</a></li><li><a class="tocitem" href="../poisson/">Poisson Likelihood (Scaled Logistic Link)</a></li><li><a class="tocitem" href="../studentt/">Student-T Likelihood</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/bernoulli/">Binary Classification</a></li><li><a class="tocitem" href="../../examples/categorical/">Multi-Class Classification</a></li><li><a class="tocitem" href="../../examples/laplace/">Regression with Laplace noise</a></li><li><a class="tocitem" href="../../examples/negativebinomial/">Event counting with Negative Binomial</a></li><li><a class="tocitem" href="../../examples/poisson/">Poisson regression</a></li><li><a class="tocitem" href="../../examples/studentt/">Regression with Student-t noise</a></li></ul></li><li><a class="tocitem" href="../../misc/">Misc</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Likelihoods</a></li><li class="is-active"><a href>Categorical Likelihood (LogisticSoftmax Link)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Categorical Likelihood (LogisticSoftmax Link)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl/blob/main/docs/src/likelihoods/categorical.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Categorical-Likelihood-(LogisticSoftmax-Link)"><a class="docs-heading-anchor" href="#Categorical-Likelihood-(LogisticSoftmax-Link)">Categorical Likelihood (LogisticSoftmax Link)</a><a id="Categorical-Likelihood-(LogisticSoftmax-Link)-1"></a><a class="docs-heading-anchor-permalink" href="#Categorical-Likelihood-(LogisticSoftmax-Link)" title="Permalink"></a></h1><p>The <a href="https://juliagaussianprocesses.github.io/GPLikelihoods.jl/stable/api/#GPLikelihoods.CategoricalLikelihood"><code>CategoricalLikelihood</code></a> with <span>$K$</span> categories with a logistic-softmax link is defined as</p><p class="math-container">\[p(y=k|\{f_j\}_{j=1}^K) = \frac{\theta_k\sigma(f_k)}{\sum_{j=1}^K \theta_j\sigma(f_j)},\]</p><p>Note that two versions are possible:</p><ul><li>The bijective one where we have <span>$K-1$</span> latent Gaussian Processes and set the last <span>$f_K$</span> to a fixed value <span>$C$</span>.</li><li>The non-bijective, or over-parametrized, version where we have <span>$K$</span> latent GPs.</li></ul><p>To call:</p><ul><li>the first one, build: <code>CategoricalLikelihood(BijectiveSimplex(LogisticSoftmaxLink(zeros(nclass))))</code></li><li>the second one : <code>CategoricalLikelihood(LogisticSoftmaxLink(zeros(nclass)))</code>.</li></ul><p>For ease of computation, we one-hot encode the labels as <span>$\boldsymbol{Y}$</span> where <span>$Y_j^i = y^i == j$</span>.</p><h2 id="The-augmentation"><a class="docs-heading-anchor" href="#The-augmentation">The augmentation</a><a id="The-augmentation-1"></a><a class="docs-heading-anchor-permalink" href="#The-augmentation" title="Permalink"></a></h2><h3 id="Bijective-version"><a class="docs-heading-anchor" href="#Bijective-version">Bijective version</a><a id="Bijective-version-1"></a><a class="docs-heading-anchor-permalink" href="#Bijective-version" title="Permalink"></a></h3><p>We have <span>$\sigma(f_K) = \sigma(C) = D \in [0, 1]$</span>.</p><p class="math-container">\[\begin{align*}
    p(y^i=k|\{\boldsymbol{f}_j\}_{j=1}^{K-1},\bf \theta) =&amp; \frac{\theta_k\sigma(f^i_k)}{\theta_K D + \sum_{j=1}^{K-1}\theta_j\sigma(f^i_j)}\\
    =&amp;\frac{\theta_k\sigma(f^i_k)}{D}\frac{1}{1 + \frac{1}{\theta_K D} \sum_{j=1}^{K-1}\theta_j\sigma(f^i_j)}\\
    =&amp; \frac{\sigma(f^i_k)}{\theta_k D}\int_{0}^\infty \exp(-\lambda \sum_{j=1}^{K-1}\theta_j\sigma(f^i_j))\operatorname{Ga}(\lambda|1,\frac{1}{\theta_K D})d\lambda\\
    p(y^i=k|\{f^i_j\}_{j=1}^{K-1},\lambda,\bf \theta) =&amp; \theta_k\sigma(f^i_k)\exp(\lambda \sum_{i=j}^{K-1}\theta_j\sigma(f^i_j))\operatorname{Ga}(\lambda|1,\frac{1}{\theta_K D})\\
    =&amp; \theta_k \sigma(f^i_k)\prod_{j=1}^{K-1}\sum_{n^i_j=0}^\infty\sigma^{n^i_j}(-f^i_j)\operatorname{Po}(n^i_j|\theta_j\lambda)\operatorname{Ga}(\lambda|1,\frac{1}{\theta_K D})\\
    p(y=k|\{\boldsymbol{f}_j\}_{j=1}^{K-1},\lambda, \{n^i_j\}_{j=1}^{K-1}) =&amp;\theta_k\sigma(f^i_k)\prod_{j=1}^{K-1}\sigma^{n^i_j}(-f^i_j)\operatorname{Po}(n^i_j|\theta_j\lambda)\operatorname{Ga}(\lambda|1,\frac{1}{\theta_K D})
\end{align*}\]</p><p>We could continue with this, but one can actually notice that a product of independent Poisson variables with a Gamma prior will produce a Negative Multinomial variable by marginalizing out <span>$\lambda$</span>:</p><p class="math-container">\[    p(y^i=k|\{\boldsymbol{f}_j\}_{j=1}^{K-1},\boldsymbol{n}^i) =\sigma(f^i_k)\prod_{j=1}^{K-1}\sigma^{n^i_j}(-f^i_j)\operatorname{NM}(\boldsymbol{n}^i|1, \left\{\frac{\theta_j}{\theta_K D + \sum_{l=1}^{K-1} \theta_l}\right\}_{j=1}^{K-1})\]</p><p>It&#39;s one less variable! Now we can easily perform the last augmentation using the Pólya-Gamma augmentations.</p><p class="math-container">\[\begin{align*}
    p(y^i=k|\{\boldsymbol{f}_j\}_{j=1}^{K=1},\boldsymbol{n}^i, \boldsymbol{\omega}^i,\bf \theta) =&amp;\operatorname{NM}(\boldsymbol{n}^i|1, \left\{\frac{\theta_j}{D + \sum_{l=1}^{K=1}\theta_l}\right\}_{j=1}^{K-1})\\
    &amp;\times \prod_{j=1}^{K-1}2^{-(y^i_j + n^i_j)}e^{\frac{1}{2}\left((y^i_j - n^i_j) f^i_j - \omega^i_j (f^i_j)^2\right)}\operatorname{PG}(\omega^i_j|y^i_j + n^i_j,0)
\end{align*}\]</p><h3 id="Non-bijective-version"><a class="docs-heading-anchor" href="#Non-bijective-version">Non-bijective version</a><a id="Non-bijective-version-1"></a><a class="docs-heading-anchor-permalink" href="#Non-bijective-version" title="Permalink"></a></h3><p>This time there is no constant to take care of!</p><p class="math-container">\[\begin{align*}
    p(y^i=k|\{\boldsymbol{f}_j\}_{j=1}^{K},\bf \theta) =&amp; \frac{\theta_k\sigma(f^i_k)}{\sum_{i=1}^{K}\theta_j\sigma(f^i_j)}\\
    =&amp;\theta_k\sigma(f^i_k)\int_0^\infty e^{-\lambda \sum_{j=1}^{K}\theta_j\sigma(f^i_j)}d\lambda\\
    p(y^i=k|\{f^i_j\}_{j=1}^{K},\bf \theta, \lambda) =&amp; \theta_k\sigma(f^i_k)\exp(\lambda \sum_{j=1}^{K-1}\theta_j\sigma(f^i_j))\\
    =&amp; \theta_k\sigma(f^i_k)\prod_{j=1}^{K}\sum_{n^i_j=0}^\infty\sigma^{n^i_j}(-f^i_j)\operatorname{Po}(n^i_j|\theta_j\lambda)\\
    p(y=k|\{\boldsymbol{f}_j\}_{j=1}^{K}, \bf \theta, \lambda, \{n^i_j\}_{j=1}^{K}) =&amp;\theta_k \sigma(f^i_k)\prod_{j=1}^{K}\sigma^{n^i_j}(-f^i_j)\operatorname{Po}(n^i_j|\theta_j\lambda)
\end{align*}\]</p><p>Note that as opposed to the bijective version, the prior on <span>$\lambda$</span> is the improper prior <span>$1_{[0, \infty]}$</span>. Similarly to the bijective version, we can also recover a Negative Multinomial</p><p class="math-container">\[    p(y^i=k|\{\boldsymbol{f}_j\}_{j=1}^{K},\boldsymbol{n}^i) =\sigma(f^i_k)\prod_{j=1}^K \widetilde{\operatorname{NM}}(\boldsymbol{n}^i|1, \left\{\frac{\theta_j\sigma(-f_j^i)}{\sum_j \theta_j}\right\}_{j=1}^{K})\]</p><p><span>$\widetilde{\operatorname{NM}}$</span> is normalizable but since <span>$p_0 = 1 - \sum_j \frac{\sigma(-f_j^i)}{K}$</span> we simply consider the non-normalized version. For the last step:</p><p class="math-container">\[    p(y^i=k|\{\boldsymbol{f}_j\}_{j=1}^{K},\boldsymbol{n}^i, \boldsymbol{\omega}^i) =\widetilde{\operatorname{NM}}(\boldsymbol{n}^i|1, \left\{\frac{\theta_j}{\sum_l \theta_l}\right\}_{j=1}^{K})\prod_{j=1}^{K}2^{-(y^i_j + n^i_j)}e^{\frac{1}{2}\left((y^i_j - n^i_j) f^i_j - \omega^i_j (f^i_j)^2\right)}\operatorname{PG}(\omega^i_j|y^i_j + n^i_j,0),\]</p><p>where we extracted the <span>$\sigma(-f_j^i)$</span> terms from the Negative Multinomial. Note that it is the same as the bijective version but with a different prior on <span>$\boldsymbol{n}$</span>.</p><h2 id="Conditional-distributions-(Sampling)"><a class="docs-heading-anchor" href="#Conditional-distributions-(Sampling)">Conditional distributions (Sampling)</a><a id="Conditional-distributions-(Sampling)-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-distributions-(Sampling)" title="Permalink"></a></h2><p>Let&#39;s define the set of all variables <span>$\boldsymbol{F} = \{\boldsymbol{f}_j\}_{j=1}^{K}$</span>, <span>$\boldsymbol{N} = \{\boldsymbol{n}^i\}_{i=1}^N$</span> and <span>$\boldsymbol{\Omega} = \{\boldsymbol{\omega}^i\}_{i=1}^N$</span>.</p><p>We are interested in the full-conditionals <span>$p(\boldsymbol{f}_j|\boldsymbol{Y}, \boldsymbol{\Omega}, \boldsymbol{N})$</span> and <span>$p(\boldsymbol{\omega}^i, \boldsymbol{n}^i|\boldsymbol{Y},\boldsymbol{F})$</span></p><p class="math-container">\[\begin{align*}
    p(\boldsymbol{f}_j|\boldsymbol{Y},\boldsymbol{\Omega}, \boldsymbol{N}) =&amp; \mathcal{N}(\boldsymbol{f}_j|\mu_j,\Sigma_j)\\
    \Sigma_j =&amp; \left(K^{-1} + \operatorname{Diagonal}(\boldsymbol{\omega}_j)\right)^{-1}\\
    \mu_j =&amp; \Sigma_j\left(\frac{\boldsymbol{y}_j - \boldsymbol{n}_j}{2} + K^{-1}\boldsymbol{\mu}_0\right)\\
\end{align*}\]</p><p>For the <strong>bijective version</strong></p><p class="math-container">\[\begin{align*}
p(\boldsymbol{\omega}^i, \boldsymbol{n}^i|\boldsymbol{y}^i, \boldsymbol{F}) =&amp; \prod_{j=1}^{K-1}\operatorname{PG}(\omega^i_j|y^i_j + n^i_j, |f^i_j|)\operatorname{NM}\left(\boldsymbol{n}^i|1, \left\{\frac{\sigma(-f^i_j)}{D + K - 1}\right\}_{j=1}^{K-1}\right)
\end{align*}\]</p><p>For the <strong>non-bijective version</strong></p><p class="math-container">\[\begin{align*}
    p(\boldsymbol{\omega}^i, \boldsymbol{n}^i|\boldsymbol{y}^i, \boldsymbol{F}) =&amp; \prod_{j=1}^{K-1}\operatorname{PG}(\omega^i_j|y^i_j + n^i_j, |f^i_j|)\operatorname{NM}\left(\boldsymbol{n}^i|1, \left\{\frac{\sigma(-f^i_j)}{K}\right\}_{j=1}^{K-1}\right)
\end{align*}\]</p><p>Note that <span>$p(\boldsymbol{\omega}^i, \boldsymbol{n}^i| \boldsymbol{Y},\boldsymbol{F})$</span> is defined in the package as a <a href="../../misc/#AugmentedGPLikelihoods.SpecialDistributions.PolyaGammaNegativeMultinomial"><code>AugmentedGPLikelihoods.SpecialDistributions.PolyaGammaNegativeMultinomial</code></a> distribution.</p><h2 id="Variational-distributions-(Variational-Inference)"><a class="docs-heading-anchor" href="#Variational-distributions-(Variational-Inference)">Variational distributions (Variational Inference)</a><a id="Variational-distributions-(Variational-Inference)-1"></a><a class="docs-heading-anchor-permalink" href="#Variational-distributions-(Variational-Inference)" title="Permalink"></a></h2><p>We define the variational distribution with a block mean-field approximation:</p><p class="math-container">\[    q(\boldsymbol{F}, \boldsymbol{\Omega}, \boldsymbol{N}) = \prod_{j=1}^{K-1} \mathcal{N}(\boldsymbol{f}_j|\boldsymbol{\mu}_j,\boldsymbol{\Sigma}_j)\prod_{i=1}^N \operatorname{NM}(\boldsymbol{n}^i|1, \boldsymbol{p}^i)\prod_{j=1}^{K-1}\operatorname{PG}(\omega^i_j|y^i_j + n^i_j, c^i_j).\]</p><p>The optimal variational parameters are given by:</p><p class="math-container">\[\begin{align*}
    c^i_j =&amp; \sqrt{(\mu^i_j)^2 + S^{ii}_j},\\
    \Sigma_j =&amp; \left(K^{-1} + \operatorname{Diagonal}(\theta_j)\right)^{-1},\\
    \mu_j =&amp; \Sigma_j\left(\frac{y - \gamma_j}{2} + K^{-1}\mu_0\right),
\end{align*}\]</p><p>where <span>$\gamma_j^i = E_{q(\boldsymbol{p})}[n_j^i] = \frac{p_j^i}{1 - \sum_{j=1}^{K-1} p_j^i}$</span>, <span>$\theta_j^i = E_{q(\omega^i_j,n^i_j)}[\omega_j^i] = \frac{y_j^i+\gamma^i_j}{2c_j^i}\tanh\left(\frac{c_j^i}{2}\right)$</span> and <span>$\widetilde{\sigma}(q(f_j^i)) = \frac{e^{-\mu_j^i}/2}{2\cosh(c_j^i/2)}$</span> which is an approximation of the expectation of <span>$E_{q(f_j^i)}[\sigma(-f_j^i)]$</span>.</p><p>For the <strong>bijective version</strong></p><p class="math-container">\[p^i_j = \frac{\widetilde{\sigma}(q(f_i^j))}{D + K - 1},\]</p><p>and for the <strong>non-bijective version</strong></p><p class="math-container">\[p^i_j = \frac{\widetilde{\sigma}(q(f_i^j))}{K},\]</p><p>We get the ELBO as</p><p class="math-container">\[\begin{align*}
    \mathcal{L} =&amp; \sum_{i=1}^N\sum_{j=1}^K -  (y^i + \gamma^i_j) \log 2 + \frac{(y_j^i - \gamma^i_j) m^i_j}{2} - \frac{(m^i_j)^2 + S_j^{ii}}{2}\theta^i_j\\ 
    &amp;- \operatorname{KL}(q(\boldsymbol{\Omega},\boldsymbol{N})||p(\boldsymbol{\Omega},\boldsymbol{N}|\boldsymbol{Y})) - \operatorname{KL}(q(\boldsymbol{F})||p(\boldsymbol{F})),
\end{align*}\]</p><p>where</p><p class="math-container">\[\begin{align*}
    \operatorname{KL}(\prod_{j} q(\omega^i_j|n^i_j)q(\bm n^i)||\prod_j p(\omega^i_j|\bm n^i, \boldsymbol{y}^i)p(\bm n^i)) =&amp; \operatorname{KL}(q(\bm n^i)||p(\bm n^i)) + \sum_j E_{q(\bm n^i)}\left[\operatorname{KL}(q(\omega^i_j|\bm n^i)||p(\omega^i_j|\bm n^i, \boldsymbol{y}^i)\right],\\
    E_{q(\bm n^i)}\left[\operatorname{KL}(q(\omega^i_j|\bm n^i)||p(\omega^i_j|n^i_j, y^i)\right] =&amp; (y^i + \gamma^i_j)\log\cosh \left(\frac{c^i_j}{2}\right) - (c^i_j)^2 \frac{\theta^i_j}{2}.
\end{align*}\]</p><p>For the <strong>bijective version</strong></p><p class="math-container">\[\operatorname{KL}(q(\bm n_i)||p(\bm n_i)) = \log p_0^q - \log p_0^p + \sum_j \gamma_i (\log p_i^q - \log p_i^p),\]</p><p>while for the <strong>non-bijective version</strong></p><p class="math-container">\[\operatorname{KL}(q(\bm n_i)||\tilde{p}(\bm n_i)) = \log p_0^q + \sum_j \gamma_i (\log p_i^q - \log p_i^p),\]</p><p>Note that we ignored the <span>$p_0^p$</span> term since we were working with the unnormalized version of the distribution.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../bernoulli/">« Bernoulli Likelihood (Logistic Link)</a><a class="docs-footer-nextpage" href="../laplace/">Laplace Likelihood »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Sunday 16 October 2022 01:57">Sunday 16 October 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
